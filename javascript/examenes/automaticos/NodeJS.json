[{"id":"q1","question":"When building a highly scalable todo application with NodeJS, what is the most effective strategy to handle long-running operations, such as image processing for todo item attachments, without blocking the event loop?","options":["a) Implement a worker pool using `worker_threads` for CPU-bound tasks.","b) Use `child_process.fork()` to spawn new processes for each long operation.","c) Offload the tasks to a dedicated message queue (e.g., RabbitMQ, SQS) and process them with separate worker services.","d) Utilize Node.js's asynchronous I/O capabilities directly, as it handles concurrency automatically."],"answer":"c","explanation":"For truly long-running, potentially CPU-intensive or high-volume tasks like image processing, offloading to a message queue and dedicated worker services is the most robust and scalable solution. It decouples the web server from the processing, provides fault tolerance, and allows for independent scaling of workers.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"`worker_threads` are excellent for CPU-bound tasks within a single Node.js process, but for very long operations or high throughput, they can still consume significant resources on the main server and don't provide the same level of decoupling and fault tolerance as a message queue system.","b":"`child_process.fork()` spawns new Node.js processes, which adds overhead for each operation. While it isolates the task, managing a large number of child processes for frequent, long-running tasks can become resource-intensive and complex, and lacks built-in queuing/retries.","d":"Node.js's asynchronous I/O is for non-blocking I/O operations (like network requests, file system access), not for CPU-bound computations. Directly performing long-running computations in the event loop will block it and make the application unresponsive."}},{"id":"q2","question":"Consider a todo application where real-time updates are critical, like notifying users when a todo item's status changes. Which Node.js technology is best suited for efficient, bidirectional communication?","options":["a) RESTful API with frequent polling.","b) Server-Sent Events (SSE).","c) WebSockets (e.g., Socket.IO).","d) Long Polling."],"answer":"c","explanation":"WebSockets provide full-duplex, bidirectional communication channels over a single TCP connection, making them ideal for real-time applications requiring frequent updates from both the server and client, such as collaborative todo lists or instant notifications.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Frequent polling is inefficient and generates excessive HTTP requests, leading to higher latency and server load for real-time updates.","b":"SSE is good for one-way (server-to-client) real-time updates. While suitable for some push notifications, it doesn't allow the client to send real-time data back to the server over the same persistent connection, which is often needed in interactive todo apps (e.g., marking complete).","d":"Long Polling is an improvement over short polling but still relies on HTTP requests, which are less efficient than WebSockets for continuous, bidirectional real-time communication. It involves the client making a request that the server holds open until new data is available or a timeout occurs, then closes and the client immediately re-initiates."}},{"id":"q3","question":"When designing a resilient todo microservice architecture with Node.js, what pattern helps prevent cascading failures when a downstream dependency (e.g., a user service) becomes unavailable?","options":["a) Circuit Breaker pattern.","b) Saga pattern.","c) Request-Response pattern.","d) Event Sourcing pattern."],"answer":"a","explanation":"The Circuit Breaker pattern prevents a system from repeatedly trying to access a failing service, thereby giving the failing service time to recover and preventing a cascading failure throughout the system. It quickly 'fails fast' by returning an error instead of blocking or waiting indefinitely.","difficulty":"avanzado","category":"Generated","invalidOptions":{"b":"The Saga pattern is used for managing distributed transactions to maintain data consistency across multiple services when a single atomic transaction is not possible, not primarily for preventing cascading failures due to service unavailability.","c":"The Request-Response pattern describes a fundamental communication interaction, but it doesn't inherently provide resilience against downstream failures.","d":"Event Sourcing is a data persistence pattern where all changes to application state are stored as a sequence of immutable events, not a pattern for handling runtime service failures."}},{"id":"q4","question":"For a high-performance todo application, which database type would generally be preferred for storing highly normalized todo item data with complex relationships (e.g., users, categories, tags) if strong ACID properties are a primary concern?","options":["a) MongoDB (Document Database).","b) Redis (Key-Value Store).","c) PostgreSQL (Relational Database).","d) Apache Cassandra (Column-Family Database)."],"answer":"c","explanation":"PostgreSQL, a relational database, excels at managing highly normalized data with complex relationships using joins and transactions. It provides strong ACID (Atomicity, Consistency, Isolation, Durability) guarantees, which are crucial when data integrity and complex querying are paramount.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"MongoDB is a NoSQL document database, good for flexible schemas and horizontal scalability, but less suited for highly normalized data with complex joins where ACID properties are critical across multiple collections.","b":"Redis is an in-memory key-value store, excellent for caching, session management, and real-time analytics due to its speed, but not designed as a primary persistent store for complex relational data.","d":"Apache Cassandra is a distributed NoSQL database designed for high availability and linear scalability, optimized for write-heavy workloads and wide-column storage. It trades strong consistency for availability and eventual consistency, making it less ideal when strict ACID properties and complex ad-hoc queries are the main requirements."}},{"id":"q5","question":"When implementing user authentication and authorization for a todo application, what is a key security consideration for handling user passwords in a Node.js backend?","options":["a) Store passwords as plain text in the database for easy retrieval.","b) Use a fast, unsalted hashing algorithm like MD5 for password storage.","c) Hash passwords using a strong, slow, salted algorithm like bcrypt or Argon2.","d) Encrypt passwords with a symmetric key and store the key in source code."],"answer":"c","explanation":"Hashing passwords with a strong, slow, and salted algorithm like bcrypt or Argon2 is crucial. 'Slow' algorithms make brute-force attacks computationally expensive, and 'salting' (adding a random string before hashing) prevents rainbow table attacks and ensures that identical passwords produce different hashes.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Storing passwords in plain text is an extreme security vulnerability, making them immediately accessible if the database is compromised.","b":"MD5 is a cryptographically broken hash function, unsuitable for password storage due to its speed and vulnerability to collision and rainbow table attacks. Lack of salting further exacerbates this risk.","d":"Encrypting passwords is less secure than hashing because encryption is reversible (meaning the original password can be recovered), and storing the encryption key in source code (or any easily discoverable location) defeats the purpose, as an attacker gaining access to the code can decrypt all passwords."}},{"id":"q6","question":"To enhance the performance of a read-heavy todo list API, specifically for frequently accessed, static data like user profiles or common lookup values (e.g., todo priorities), what caching strategy is most appropriate for a Node.js application?","options":["a) Client-side caching using browser Local Storage.","b) In-memory caching within the Node.js process using a simple JavaScript object.","c) Distributed caching using an external service like Redis.","d) Database-level caching provided by the ORM."],"answer":"c","explanation":"For high-performance, scalable Node.js applications, especially in a distributed environment (multiple Node.js instances or microservices), a distributed cache like Redis is superior. It provides shared access to cached data across all instances, high read/write performance, persistence options, and advanced features like pub/sub, ensuring consistency and efficiency.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Client-side caching is useful but only benefits individual users and doesn't reduce the load on the backend for other users or improve server-side processing performance.","b":"In-memory caching (simple JavaScript object) is fast but limited to the memory of a single Node.js process. It doesn't scale across multiple instances, and cached data is lost if the process restarts. It's generally suitable only for very small, non-critical, or short-lived caches.","d":"While ORMs might offer some form of query caching, it's typically less flexible, less performant, and harder to manage for generic data caching compared to a dedicated distributed cache solution like Redis. It's also tied to the database interactions, not a general-purpose application cache."}},{"id":"q7","question":"A todo application needs to ensure that only authenticated users can access and modify their own todo items. Which authorization mechanism is most suitable for a stateless Node.js API?","options":["a) Session-based authentication stored in server memory.","b) JWT (JSON Web Token) based authentication.","c) Basic HTTP Authentication.","d) OAuth 1.0."],"answer":"b","explanation":"JWTs are ideal for stateless APIs because they contain all the necessary user information (including roles and permissions) within the token itself, signed by the server. This allows the server to verify authenticity and authorize requests without needing to store session data or query a database for every request, making it highly scalable and suitable for microservices.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Session-based authentication requires the server to store session state, making the API stateful and more challenging to scale horizontally across multiple instances (unless a distributed session store is used, which adds complexity).","c":"Basic HTTP Authentication sends credentials with every request, typically unencrypted over HTTP (unless TLS is used), and often involves re-authentication for each request, which is inefficient and less secure for a modern web application.","d":"OAuth 1.0 is an older authorization framework primarily designed for delegated authorization (e.g., allowing a third-party app to access user data on another service), not typically used for direct user authentication within a single application's API, and it is more complex than JWT for this purpose. OAuth 2.0 is more common now."}},{"id":"q8","question":"What is the primary benefit of using `async/await` over raw Callbacks or Promises `.then().catch()` in complex Node.js todo application logic involving multiple sequential asynchronous operations?","options":["a) It eliminates the need for error handling.","b) It makes asynchronous code appear and behave more like synchronous code, improving readability and maintainability.","c) It significantly reduces the execution time of asynchronous operations.","d) It automatically handles race conditions in concurrent operations."],"answer":"b","explanation":"`async/await` is syntactic sugar built on Promises. Its main advantage is that it allows developers to write asynchronous code in a sequential, synchronous-like style, which drastically improves readability and reduces callback hell or promise chain nesting, making complex asynchronous flows much easier to understand and debug.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"`async/await` does not eliminate error handling; instead, it allows standard `try...catch` blocks to be used for error handling, making it more familiar for developers coming from synchronous programming paradigms. Unhandled promise rejections (which `await` expressions are built on) still need to be caught.","c":"`async/await` does not inherently reduce execution time. It is a syntax transformation over Promises and the event loop. The underlying asynchronous operations still take the same amount of time; it only changes how the code is structured and managed.","d":"`async/await` does not automatically handle race conditions. Race conditions occur when multiple concurrent operations try to access and modify shared resources, and their outcome depends on the non-deterministic order of execution. Preventing race conditions requires explicit synchronization mechanisms, careful design, or specific libraries, not just `async/await`."}},{"id":"q9","question":"When deploying a Node.js todo application to production, what is the best practice for managing environment-specific configurations (e.g., database credentials, API keys) securely?","options":["a) Hardcode sensitive data directly into the source code.","b) Store all configurations in a `.env` file and commit it to version control.","c) Use environment variables set by the deployment environment (e.g., Docker, Kubernetes, CI/CD pipeline).","d) Store configurations in a public JSON file accessible to the application."],"answer":"c","explanation":"Environment variables are the most secure and flexible way to manage environment-specific configurations in production. They keep sensitive data out of source control, allow for easy changes across different environments (development, staging, production), and are natively supported by containerization and orchestration tools.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Hardcoding sensitive data is a major security vulnerability, as it exposes credentials to anyone with access to the source code.","b":"While `.env` files are commonly used in development, committing them to version control, especially with sensitive data, is a security risk. They should be `.gitignore`d and only used locally or with specific environment variable loading mechanisms that do not expose them.","d":"Storing configurations in a public JSON file is highly insecure, as it makes sensitive data accessible to anyone who can access the file, potentially even via the web server."}},{"id":"q10","question":"A Node.js todo application uses a PostgreSQL database. To prevent SQL Injection attacks when accepting user input for filtering todo items, which method is the most secure and recommended?","options":["a) Concatenate user input directly into SQL queries after basic validation.","b) Use parameterized queries (prepared statements).","c) Sanitize user input by replacing problematic characters with an empty string.","d) Escape single quotes in user input using a custom function."],"answer":"b","explanation":"Parameterized queries (or prepared statements) are the gold standard for preventing SQL Injection. They separate the SQL query structure from the user-provided data. The database engine then processes the query and the data separately, ensuring that user input is treated as literal values, not executable code, regardless of its content.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Concatenating user input directly into SQL queries is the primary cause of SQL Injection vulnerabilities, even with basic validation, as malicious input can still alter query logic.","c":"Sanitizing user input by replacing characters is a dangerous and unreliable method. Attackers can often bypass such sanitization filters using various encoding tricks or by finding characters that were overlooked. It's an incomplete and fragile defense.","d":"Escaping single quotes is a common but insufficient defense. While it handles basic cases, it is prone to errors, requires careful implementation for all possible attack vectors, and is significantly less robust than parameterized queries against various forms of injection."}},{"id":"q11","question":"What is the purpose of the `process.nextTick()` function in Node.js, and how does it relate to the event loop in the context of a todo application's request handling?","options":["a) It schedules a callback to be executed after all I/O operations in the current event loop iteration, but before `setImmediate()`.","b) It schedules a callback to be executed immediately in the next turn of the event loop's 'check' phase.","c) It schedules a callback to be executed at the beginning of the next event loop iteration, before any other phases.","d) It schedules a callback to be executed as soon as the current operation completes, effectively deferring it to the very next tick before I/O or timers."],"answer":"d","explanation":"`process.nextTick()` callbacks are placed at the head of the event queue for the current 'tick'. This means they run immediately after the current operation finishes executing, even before any I/O callbacks, `setImmediate()`, or `setTimeout()` callbacks for the current or next event loop iteration. It's often used to defer execution just enough to break up large synchronous tasks or to ensure certain operations occur before other asynchronous ones.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"This describes the behavior of `setImmediate()`, not `process.nextTick()`. `setImmediate()` runs in the 'check' phase, after I/O callbacks.","b":"This is incorrect. `process.nextTick()` runs 'in the current tick' as soon as the current call stack clears, whereas `setImmediate()` runs in the 'check' phase of the next event loop cycle.","c":"While it does run early, it's more accurate to say it runs *within* the current event loop iteration, immediately after the current synchronous code finishes, rather than at the beginning of the 'next' iteration in the typical sense of the event loop phases (timers, I/O, poll, check, close)."}},{"id":"q12","question":"When designing a highly available and fault-tolerant Node.js todo backend, how can 'sticky sessions' affect horizontal scaling and what's a common alternative?","options":["a) Sticky sessions enhance scalability by distributing requests evenly; an alternative is using a shared database for session storage.","b) Sticky sessions are required for stateless APIs; an alternative is JWT authentication.","c) Sticky sessions hinder horizontal scalability by tying a user to a specific server; a common alternative is a distributed session store or stateless tokens like JWT.","d) Sticky sessions are primarily for cache invalidation; an alternative is a CDN."],"answer":"c","explanation":"Sticky sessions (where a load balancer ensures a user's requests always go to the same server) hinder horizontal scalability because they prevent true load distribution and make server failures more impactful (user loses session). A common alternative is a distributed session store (like Redis) or, even better for statelessness, using JWTs, where session state is entirely managed on the client side with cryptographically signed tokens.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Sticky sessions do not enhance scalability by distributing requests evenly; they do the opposite by forcing requests from a single user to a single server instance. While a shared database can store session data, it is a solution to allow statelessness (no sticky sessions), not an alternative if sticky sessions were good for scalability.","b":"Sticky sessions are used for *stateful* APIs to maintain session state on a specific server instance. Stateless APIs, by definition, avoid storing server-side session state and often use JWTs precisely to avoid the need for sticky sessions.","d":"Sticky sessions have no direct relation to cache invalidation. CDNs (Content Delivery Networks) are used for static asset caching and distribution, not for managing application session state."}},{"id":"q13","question":"A todo application needs to generate complex, dynamic PDF reports of user todo lists. Given that PDF generation can be CPU-intensive and synchronous in nature for certain libraries, what is the optimal Node.js approach to prevent this from blocking the main event loop?","options":["a) Use `setTimeout(..., 0)` to defer the PDF generation function.","b) Run the PDF generation code within a `cluster` worker process.","c) Utilize Node.js `worker_threads` to perform the PDF generation in a separate thread.","d) Implement a recursive `setImmediate()` loop to chunk the PDF generation."],"answer":"c","explanation":"`worker_threads` are designed for CPU-bound tasks in Node.js. They allow you to run JavaScript code in parallel threads, completely separate from the main event loop. This ensures that a heavy, synchronous operation like complex PDF generation does not block the main thread, keeping the server responsive.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"`setTimeout(..., 0)` (or `setImmediate()`) only defers the execution to the next tick or iteration of the event loop. The operation, when it runs, will still block the event loop for its duration if it's synchronous and CPU-intensive.","b":"While `cluster` workers run in separate processes, they are primarily for distributing incoming network connections across multiple CPU cores to improve throughput for I/O-bound tasks. A single CPU-bound task inside one of these workers would still block that specific worker's event loop. `worker_threads` are more granular for specific CPU-bound computations *within* a process.","d":"Implementing a recursive `setImmediate()` loop to chunk the operation requires the underlying library to support incremental processing and pausing, which is rarely the case for complex PDF generation. If the library is synchronous, this approach won't prevent blocking during the chunk processing."}},{"id":"q14","question":"For a large-scale todo application, how can a Node.js backend effectively handle schema migrations (e.g., adding a new field to a 'todos' table) in a production environment with minimal downtime?","options":["a) Perform a 'big bang' migration where the application is taken offline, the schema is updated, and then the application is brought back online.","b) Use a blue/green deployment strategy combined with incremental, backward-compatible schema changes.","c) Manually update the database schema via a direct SQL client during peak hours.","d) Rely solely on ORM auto-migrations which automatically handle all schema changes during application startup."],"answer":"b","explanation":"Blue/green deployment, combined with a strategy of making schema changes backward-compatible (e.g., adding nullable columns first, then populating, then enforcing non-null constraints in a later deployment), allows for zero-downtime migrations. The new 'green' version with updated schema logic is deployed alongside the old 'blue' version, traffic is slowly shifted, and only once 'green' is stable is 'blue' decommissioned.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"A 'big bang' migration causes significant downtime, which is unacceptable for large-scale production applications.","c":"Manually updating during peak hours is extremely risky, prone to errors, and can lead to immediate downtime or data inconsistencies. It lacks automation, rollback capabilities, and proper testing.","d":"While ORM auto-migrations are convenient in development, relying solely on them for production can be dangerous. They might not generate optimal or safe SQL for zero-downtime migrations, can be irreversible, and often lack the fine-grained control needed for complex production environments. Manual review and controlled application of migrations (often with a dedicated migration tool) are preferred."}},{"id":"q15","question":"In a Node.js todo application that integrates with various third-party APIs (e.g., calendar services, email notifications), what is a robust strategy to handle rate limiting imposed by these external services?","options":["a) Implement aggressive retry logic with exponential backoff for all API calls.","b) Use a queueing mechanism (e.g., a message queue like RabbitMQ) with a rate-limiting consumer.","c) Perform all third-party API calls synchronously to avoid hitting limits.","d) Ignore rate limits and hope for the best, letting calls fail and rely on client-side retries."],"answer":"b","explanation":"A queueing mechanism with a rate-limiting consumer is the most robust solution. Requests to external APIs are pushed onto a queue, and a dedicated worker consumes messages from the queue at a controlled rate (respecting the third-party API's limits). This prevents hitting limits, absorbs bursts of requests, provides durability, and allows for retry logic without blocking the main application flow.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"While exponential backoff is good for retries, aggressive retries without proper rate limiting can still quickly hit and exceed API rate limits, especially during peak loads, leading to repeated failures and potential blacklisting.","c":"Performing all third-party API calls synchronously will block the Node.js event loop, severely degrading performance and responsiveness, and does not inherently prevent hitting rate limits if the synchronous calls happen too quickly.","d":"Ignoring rate limits is a bad practice that will lead to service disruptions, blocked API access, and a poor user experience. Relying solely on client-side retries offloads the problem without solving the root cause of server-side rate limit violations."}},{"id":"q16","question":"When developing a module for a shared utility in a large Node.js monorepo (e.g., for common validation logic for todo items), what's the best practice to ensure clear API and avoid accidental internal state modification?","options":["a) Export a single mutable object that can be shared and modified across all usages.","b) Export functions and classes that operate on immutable data or return new instances, avoiding shared mutable state.","c) Use global variables to store shared state for all utility functions.","d) Keep all utility logic within the application's main entry file."],"answer":"b","explanation":"Exporting pure functions or classes that operate on immutable data (or create new instances) is a cornerstone of robust module design in Node.js. This prevents unintended side effects, makes code easier to test, reason about, and reuse, and avoids issues with shared mutable state across different parts of a large application or monorepo.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Exporting a single mutable object creates shared state, which can lead to unpredictable behavior, difficult-to-trace bugs, and race conditions if multiple parts of the application modify it concurrently.","c":"Using global variables is a very poor practice that leads to tightly coupled code, makes debugging extremely difficult, and creates a high risk of name collisions and unexpected side effects across the application.","d":"Keeping all utility logic in the main entry file defeats the purpose of modularity, making the codebase unmanageable, difficult to test, and impossible to reuse across different parts of a large application or monorepo."}},{"id":"q17","question":"To improve the cold start time and overall resource utilization of a serverless Node.js todo backend (e.g., AWS Lambda), what optimization technique is highly effective?","options":["a) Maximize the number of external dependencies (NPM packages).","b) Bundle the application code and its dependencies into a single, optimized file using tools like Webpack or Rollup.","c) Use a synchronous database connection to ensure immediate readiness.","d) Increase the memory allocation far beyond what is needed."],"answer":"b","explanation":"Bundling the application code and its dependencies into a single, optimized file (often minified and tree-shaken) significantly reduces the size of the deployment package. This directly translates to faster download times for the serverless environment and quicker parsing/loading of modules, thereby reducing cold start times.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Maximizing external dependencies increases the package size and the number of files Node.js needs to load, directly increasing cold start times. Minimizing dependencies is a key optimization.","c":"Using a synchronous database connection would block the Lambda's execution thread during database operations, leading to timeouts and poor performance, not improved cold start times. Asynchronous connections are essential for Node.js's non-blocking nature.","d":"While increasing memory *can* sometimes improve performance (by allocating more CPU power proportionally in some serverless environments like Lambda), allocating far beyond what's needed is wasteful and costly. The primary driver of cold start for Node.js is package size and module loading, not just memory."}},{"id":"q18","question":"When developing a robust Node.js API for a todo application, what is a crucial aspect of input validation to prevent common web vulnerabilities like Cross-Site Scripting (XSS) when displaying user-generated todo descriptions?","options":["a) Trust all user input as safe HTML and display it directly.","b) Use client-side JavaScript validation only, as it's sufficient for XSS prevention.","c) Sanitize HTML input on the server-side to remove or escape potentially malicious tags and attributes.","d) Encode all user input into Base64 before storing and displaying."],"answer":"c","explanation":"Server-side HTML sanitization is crucial for preventing XSS. It involves parsing user-submitted HTML and systematically removing or escaping any tags, attributes, or JavaScript event handlers that could be exploited to inject malicious scripts into the page displayed to other users. Client-side validation is easily bypassed.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Trusting all user input and displaying it directly is a direct path to XSS vulnerabilities, as attackers can embed malicious scripts that will execute in other users' browsers.","b":"Client-side validation is for user experience and basic form correctness, but it is never sufficient for security. Attackers can easily bypass client-side validation by sending malicious requests directly to the server.","d":"Encoding user input into Base64 (or similar encodings) might prevent direct execution of scripts, but it doesn't solve the problem of XSS. The browser will decode the input before rendering, and if the original content was malicious, it would still execute. Proper sanitization is about neutralizing the malicious parts of the input."}},{"id":"q19","question":"Which logging strategy is most effective for a highly distributed Node.js todo microservice architecture to enable centralized monitoring, debugging, and analytics?","options":["a) Log directly to the console (`console.log`) in each microservice.","b) Store logs in local files on each server and periodically retrieve them manually.","c) Use a structured logging library (e.g., Winston, Pino) to output JSON logs to `stdout`, and then ship them to a centralized logging system (e.g., ELK Stack, Splunk).","d) Implement custom HTTP endpoints in each service for log retrieval."],"answer":"c","explanation":"Structured logging (e.g., JSON format) to `stdout` (standard output) is the best practice for distributed systems. This allows containerization platforms (Docker, Kubernetes) and serverless environments to easily capture these logs. They can then be shipped to a centralized logging system (like ELK Stack, Grafana Loki, Splunk, Datadog) where they can be aggregated, indexed, searched, filtered, and analyzed across all services.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Logging directly to the console with `console.log` is unstructured, difficult to parse programmatically, and makes it challenging to aggregate and search logs across multiple services.","b":"Storing logs in local files is problematic for distributed systems. It complicates aggregation, searching, and real-time monitoring, and logs might be lost if containers or instances are ephemeral or fail.","d":"Implementing custom HTTP endpoints for log retrieval adds unnecessary complexity and overhead to each service, is inefficient for continuous logging, and does not provide a robust or standardized mechanism for centralized log management."}},{"id":"q20","question":"In a Node.js todo application handling a large volume of concurrent requests, what is a key architectural consideration to prevent the 'Thundering Herd' problem when a shared resource (e.g., a database connection pool) becomes available after a brief outage?","options":["a) Implement aggressive retries for all failed database connections.","b) Design the application to synchronously wait for the resource to become available.","c) Introduce Jitter to retry delays (randomized backoff) and potentially use a distributed lock or queue.","d) Increase the maximum number of database connections to an extremely high value."],"answer":"c","explanation":"The 'Thundering Herd' problem occurs when many processes or threads simultaneously contend for a resource that becomes available, leading to overload. Introducing 'Jitter' (randomized delay) to exponential backoff for retries spreads out the retry attempts, preventing all clients from retrying simultaneously. A distributed lock or queue can also manage access to a limited resource.","difficulty":"avanzado","category":"Generated","invalidOptions":{"a":"Aggressive retries without proper backoff or jitter will exacerbate the Thundering Herd problem, as all requests will hit the resource at roughly the same time once it's available, causing it to immediately go down again.","b":"Synchronously waiting for a resource to become available would block the Node.js event loop, making the application unresponsive and potentially leading to timeouts and cascading failures.","d":"Increasing the maximum number of database connections might help if the current pool is too small, but it's not a solution for the Thundering Herd problem itself. An extremely high value can put excessive load on the database server, leading to its own performance issues or crashes, rather than solving contention."}}]